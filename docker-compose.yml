services:
  #--------------------------------------------------------------------------
  # Frontend Service (React Client)
  #--------------------------------------------------------------------------
  client:
    build:
      context: ./services/client # Path to the client directory from the project root
      dockerfile: Dockerfile 
    container_name: lecturebot-client-app
    ports:
      - "3000:3000" # Maps host port 3000 to container port 3000 (from your Dockerfile EXPOSE)
    networks:
      - lecturebot-net
    restart: unless-stopped
    depends_on:
      - server # Client can start independently of the server for static assets, but requests will fail until the server is up.

  #--------------------------------------------------------------------------
  # Backend Service (Spring Boot Server)
  #--------------------------------------------------------------------------
  server:
    build:
      context: ./services/server # Path to the server directory from the project root
      dockerfile: Dockerfile
    container_name: lecturebot-app-server
    ports:
      - "8080:8080"
    environment:
      - SERVER_PORT=${SERVER_PORT:-8080}
      - SPRING_DATASOURCE_URL=jdbc:postgresql://lecturebot-db:5432/${DB_NAME:-lecturebot_db}
      - SPRING_DATASOURCE_USERNAME=${DB_USER:-lecturebot_user}
      - SPRING_DATASOURCE_PASSWORD=${DB_PASSWORD:-mysecretpassword} # Ensure this matches POSTGRES_PASSWORD below
      - SPRING_JPA_HIBERNATE_DDL_AUTO=${SPRING_JPA_HIBERNATE_DDL_AUTO:-update} # or validate for prod
      - GENAI_SERVICE_BASE_URL=http://genai-service:8001 # URL for GenAI service
      - LECTUREBOT_CLIENT_ORIGIN=${LECTUREBOT_CLIENT_ORIGIN:-http://localhost:3000} # For CORS

    depends_on:
      lecturebot-db: { condition: service_healthy }
      genai-service:
        condition: service_healthy # Wait for GenAI service to be healthy before starting
    networks:
      - lecturebot-net
    restart: unless-stopped

  #--------------------------------------------------------------------------
  # GenAI Service
  #--------------------------------------------------------------------------
  genai-service:
    build:
      context: ./services/genai # Path to the genai directory
      dockerfile: Dockerfile
    container_name: lecturebot-genai-service
    ports:
      - "8001:8001"
    environment:
      - LLM_PROVIDER=${LLM_PROVIDER:-openai} # Default to openai
      - OLLAMA_MODEL_NAME=${OLLAMA_MODEL_NAME:-llama3:8b-instruct-q4_K_M} # Ensure this matches the model pulled by ollama-setup
      - OLLAMA_BASE_URL=${OLLAMA_BASE_URL:-http://host.docker.internal:11434} # For externally run Ollama
      - OPENAI_API_KEY=${OPENAI_API_KEY:-your_openai_key_if_using_openai}
      - OPENAI_MODEL_NAME=${OPENAI_MODEL_NAME:-gpt-4o-mini}
      - TUM_AET_LLM_API_BASE=${TUM_AET_LLM_API_BASE:-https://gpu.aet.cit.tum.de/api}
      - TUM_AET_LLM_API_KEY=${TUM_AET_LLM_API_KEY:-your_tum_aet_key_here}
      - TUM_AET_LLM_MODEL_NAME=${TUM_AET_LLM_MODEL_NAME:-llama3.3:latest}
      - WEAVIATE_URL=http://weaviate:8080 # Connect to weaviate service
      - TOKENIZERS_PARALLELISM=false # Recommended for some environments
    networks:
      - lecturebot-net
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8001/health"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 30s # Give the service time to load models before checking
    depends_on:
      weaviate: { condition: service_started } # Weaviate doesn't have a simple built-in healthcheck for compose v1 style

  #--------------------------------------------------------------------------
  # Weaviate Service
  #--------------------------------------------------------------------------
  weaviate:
    image: semitechnologies/weaviate:latest
    container_name: lecturebot-weaviate-db
    ports:
      - "8081:8080" # Changed host port for Weaviate REST to 8081 to avoid conflict with server
      - "50051:50051" # gRPC
    volumes:
      - weaviate_data:/var/lib/weaviate
    environment:
      AUTHENTICATION_ANONYMOUS_ACCESS_ENABLED: 'true'
      PERSISTENCE_DATA_PATH: '/var/lib/weaviate'
      DEFAULT_VECTORIZER_MODULE: 'none' # As per your command
      ENABLE_MODULES: ''                # As per your command
      CLUSTER_HOSTNAME: 'node1'         # Recommended for Weaviate
    networks:
      - lecturebot-net
    restart: unless-stopped

  #--------------------------------------------------------------------------
  # Ollama Service
  # Ollama will be run externally. See README.md for instructions.
  #--------------------------------------------------------------------------

  #--------------------------------------------------------------------------
  # Database Service (PostgreSQL)
  #--------------------------------------------------------------------------
  lecturebot-db:
    image: postgres:16
    container_name: lecturebot-postgres-db
    environment:
      POSTGRES_DB: ${DB_NAME:-lecturebot_db}
      POSTGRES_USER: ${DB_USER:-lecturebot_user}
      POSTGRES_PASSWORD: ${DB_PASSWORD:-mysecretpassword} # This now consistently uses the same variable as the server
    ports:
      - "5432:5432" 
    volumes:
      - lecturebot_db_data:/var/lib/postgresql/data
      - ./services/server/db-init:/docker-entrypoint-initdb.d # Mount init scripts from server/db-init
    networks:
      - lecturebot-net
    restart: unless-stopped
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U lecturebot_user -d lecturebot_db"]
      interval: 10s
      timeout: 5s
      retries: 5

volumes:
  lecturebot_db_data: {}
  weaviate_data: {}      # Named volume for Weaviate data
  # ollama_data: {}      # Ollama data managed externally


networks:
  lecturebot-net:
    driver: bridge
