# GenAI Service Environment Variables
#
# Copy this file to .env in this directory for local standalone development,
# or set these variables in the root .env file for the main docker-compose setup.

# --- LLM Provider Selection ---
# Select the LLM provider to use. Options: "openai", "ollama", "tum_aet"
LLM_PROVIDER=openai

# --- OpenAI Settings ---
# Required if LLM_PROVIDER is "openai"
OPENAI_API_KEY="sk-..."
OPENAI_MODEL_NAME="gpt-4o-mini"

# --- Ollama Settings ---
# Used if LLM_PROVIDER is "ollama"
OLLAMA_BASE_URL="http://localhost:11434"
OLLAMA_MODEL_NAME="llama3:8b-instruct-q4_K_M"

# --- TUM AET GPU LLM Settings ---
# Used if LLM_PROVIDER is "tum_aet"
TUM_AET_LLM_API_BASE="https://gpu.aet.cit.tum.de/api" # Note: LangChain adds /chat/completions
TUM_AET_LLM_API_KEY="sk-..."
TUM_AET_LLM_MODEL_NAME="llama3.3:latest"

# --- Weaviate Vector Database Settings ---
WEAVIATE_URL="http://weaviate:8080" # This is the internal Docker network URL